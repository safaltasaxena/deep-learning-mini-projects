# ğŸ“‰ Customer Churn Prediction using ANN (Imbalanced Dataset Handling)

ğŸ“Œ A Deep Learning mini project that predicts **customer churn** using an
**Artificial Neural Network (ANN)** and explores **multiple techniques to handle
imbalanced datasets**.

---

## ğŸ“– About
Customer churn prediction is a critical business problem where the goal is to
identify customers who are likely to leave a service.

This project builds an **ANN-based binary classification model** and goes beyond
basic training by addressing the **class imbalance problem**, which is common in
real-world churn datasets.

---

## ğŸ“Š Dataset
Dataset used: **Telco Customer Churn Dataset**

### Target Variable:
- **Churn**  
  - `1` â†’ Customer churned  
  - `0` â†’ Customer retained  

### Key Features:
- Customer tenure  
- Monthly charges  
- Total charges  
- Contract type  
- Payment method  
- Internet & phone services  
- Demographic information  

---

## ğŸ§¹ Data Preprocessing
- Dropped irrelevant features (e.g., customer ID)
- Converted numeric columns stored as strings
- Handled missing values
- Binary encoding for Yes/No features
- One-hot encoding for multi-class categorical features
- Feature scaling using **MinMaxScaler**

---

## ğŸ“Š Exploratory Data Analysis (EDA)
- Churn distribution by **tenure**
- Churn distribution by **monthly charges**
- Visualization of churn vs non-churn customers

---

## ğŸ§  Model Architecture (ANN)
- Input layer: 26 features
- Hidden Layer 1: Dense (20 neurons, ReLU)
- Hidden Layer 2: Dense (15 neurons, ReLU)
- Output Layer: Dense (1 neuron, Sigmoid)

---

## ğŸ› ï¸ Training Configuration
- Optimizer: **Adam**
- Loss Function: **Binary Crossentropy**
- Metric: **Accuracy**
- Epochs: **100**

---

## ğŸ“ˆ Model Evaluation
- Accuracy
- Precision
- Recall
- F1-score
- Confusion Matrix visualization

Initial results show **poor recall for churned customers** due to class imbalance,
which motivates further techniques.

---

## âš ï¸ Handling Imbalanced Dataset

The project implements **four different techniques** to improve churn prediction
performance:

---

### 1ï¸âƒ£ Random Under-Sampling
- Reduces majority class samples
- Balances dataset by removing excess non-churn records
- Improves recall for minority class

---

### 2ï¸âƒ£ Random Over-Sampling
- Duplicates minority class samples
- Maintains all majority samples
- Reduces bias toward majority class

---

### 3ï¸âƒ£ SMOTE (Synthetic Minority Over-sampling Technique)
- Generates synthetic samples for minority class
- Preserves feature distribution
- Provides better generalization than simple oversampling

---

### 4ï¸âƒ£ Ensemble Method with Under-Sampling
- Splits majority class into multiple batches
- Trains multiple ANN models
- Combines predictions using majority voting
- Produces the most stable and balanced results

---

## â–¶ï¸ Usage
1. ğŸ“¥ Clone the repository  
2. ğŸ““ Open the notebook in **Google Colab** or **Jupyter Notebook**  
3. â–¶ï¸ Run all cells to:
   - Preprocess data  
   - Train ANN model  
   - Evaluate baseline performance  
   - Apply imbalance handling techniques  
   - Compare results  

---

## ğŸ§  Key Learnings
- Why accuracy alone is misleading for imbalanced data
- Importance of recall & F1-score in churn prediction
- How different imbalance techniques affect ANN performance
- How to build production-ready churn models

---

## ğŸ“Œ Conclusion
Handling imbalanced datasets is **critical** in churn prediction.
Applying resampling techniques and ensemble methods significantly improves
model performance and real-world usability.

---

ğŸš€ *Part of my Deep Learning mini projects focused on solving real-world,
business-critical problems using neural networks.*
