# ğŸ§  Neural Networks from Scratch â€“ Gradient Descent & Fundamentals  

ğŸ“Œ A Deep Learning mini project focused on understanding **how neural networks
work internally**, covering activation functions, loss functions, and
**gradient descent (batch, stochastic, and mini-batch)** through hands-on,
from-scratch implementations.

---

## ğŸ“– About  
This project dives deep into the **core building blocks of Deep Learning**.
Instead of relying only on high-level libraries, the focus is on understanding
the **mathematics and logic behind neural network training**.

The project explains:
- How activation functions transform inputs  
- How loss functions quantify error  
- How gradient descent updates weights and bias  
- How neural networks learn via backpropagation  

---

## âœ¨ Concepts Covered  

### ğŸ”¹ Activation Functions
- Sigmoid  
- Tanh  
- ReLU  
- Leaky ReLU  

### ğŸ”¹ Neural Network Training
- Forward propagation  
- Loss computation  
- Backpropagation  
- Chain rule intuition  

### ğŸ”¹ Loss / Cost Functions
- Mean Absolute Error (MAE)  
- Log Loss (Binary Crossentropy)  
- Numerical stability using epsilon  

### ğŸ”¹ Gradient Descent Variants
- Batch Gradient Descent  
- Stochastic Gradient Descent (SGD)  
- Mini-Batch Gradient Descent  
- Learning rate & convergence behavior  

---

## ğŸ“Š Mathematical Foundations
- ğŸ“ Derivatives & partial derivatives  
- ğŸ§® Matrix operations using NumPy  
- ğŸ“ˆ Dot products and vectorized computation  

---

## ğŸ› ï¸ Implementations  

### ğŸ§© From Scratch (NumPy)
- Sigmoid and Log Loss implementation  
- Gradient Descent for logistic regression  
- Custom neural network class (`myNN`)  
- Manual weight and bias updates  

### ğŸ¤– Using TensorFlow / Keras
- Logistic regression as a single-neuron neural network  
- Model training using Adam optimizer  
- Weight and bias extraction  
- Comparison with custom from-scratch implementation  

---

## ğŸ  Datasets Used  

### ğŸ§¾ Insurance Dataset
- Features:  
  - Age  
  - Affordability  
- Target:  
  - Whether a person has insurance or not  
- Used for:
  - Logistic regression  
  - Gradient descent from scratch  
  - Neural network training comparison  

---

### ğŸ˜ï¸ Bangalore Home Prices Dataset
- Features:  
  - Area  
  - Number of bedrooms  
- Target:  
  - House price  
- Used for:
  - Batch Gradient Descent  
  - Stochastic Gradient Descent  
  - Cost vs iterations comparison  

---

## ğŸ“ˆ Visualizations
- Cost vs iterations graphs  
- Batch vs Stochastic Gradient Descent comparison  
- Model convergence behavior  

---

## â–¶ï¸ Usage  
1. ğŸ“¥ Clone the repository  
2. ğŸ““ Open the notebook in **Google Colab** or **Jupyter Notebook**  
3. â–¶ï¸ Run all cells to:
   - Explore activation functions  
   - Implement loss functions  
   - Train models using different GD techniques  
   - Compare from-scratch and library-based approaches  

---

## ğŸ§  Key Learnings
- Why activation functions are critical in deep learning  
- How loss functions guide model optimization  
- How different gradient descent variants behave  
- Why feature scaling impacts convergence  
- How neural networks learn internally from scratch  

---

ğŸš€ *Part of my Deep Learning mini projects focused on mastering neural network fundamentals from scratch!*  

