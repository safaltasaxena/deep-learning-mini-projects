# ğŸ§  Neural Networks from Scratch â€“ Gradient Descent & Fundamentals  

ğŸ“Œ A Deep Learning mini project focused on understanding **how neural networks
work internally**, covering activation functions, loss functions, and
**gradient descent (batch, stochastic, and mini-batch)** through hands-on,
from-scratch implementations.

---

## ğŸ“– About  
This project dives deep into the **core building blocks of Deep Learning**.
Instead of relying only on high-level libraries, the goal is to **understand
the math and logic behind neural network training**.

It explains:
- How activation functions transform data
- How loss functions measure error
- How gradient descent updates weights
- How neural networks learn via backpropagation

---

## âœ¨ Concepts Covered  

### ğŸ”¹ Activation Functions
- Sigmoid  
- Tanh  
- ReLU  
- Leaky ReLU  

### ğŸ”¹ Neural Network Training
- Forward propagation  
- Loss calculation  
- Backpropagation  
- Chain rule intuition  

### ğŸ”¹ Loss / Cost Functions
- Mean Absolute Error (MAE)  
- Log Loss (Binary Crossentropy)  
- Numerical stability using epsilon  

### ğŸ”¹ Gradient Descent
- Batch Gradient Descent  
- Stochastic Gradient Descent (SGD)  
- Mini-Batch Gradient Descent  
- Learning rate & convergence  

---

## ğŸ“Š Mathematical Foundations
- ğŸ“ Derivatives & partial derivatives  
- ğŸ§® Matrix operations using NumPy  
- ğŸ“ˆ Dot products and vectorized computation  

---

## ğŸ› ï¸ Implementations  

### ğŸ§© From Scratch (NumPy)
- Sigmoid & Log Loss implementation  
- Gradient Descent for logistic regression  
- Custom neural network class (`myNN`)  
- Manual weight and bias updates  

### ğŸ¤– Using TensorFlow / Keras
- Logistic regression as a single-neuron network  
- Weight & bias extraction  
- Comparison with custom implementation  

---

## ğŸ  Real-World Datasets Used  
- Insurance dataset (binary classification)  
- Home price dataset (regression)  

Data preprocessing includes:
- Feature scaling  
- Trainâ€“test split  

---

## ğŸ“ˆ Visualizations
- Cost vs iterations graphs  
- Batch vs Stochastic GD comparison  
- Model convergence behavior  

---

## â–¶ï¸ Usage  
1. ğŸ“¥ Clone the repository  
2. ğŸ““ Open the notebook in **Google Colab** or **Jupyter Notebook**  
3. â–¶ï¸ Run all cells to:
   - Explore activation functions  
   - Implement loss functions  
   - Train models using different GD techniques  
   - Compare from-scratch vs library-based results  

---

## ğŸ§  Key Learnings
- Why activation functions matter  
- How loss functions guide learning  
- How gradient descent optimizes weights  
- Why scaling affects convergence  
- How neural networks learn internally  

---

ğŸš€ *Part of my Deep Learning mini projects focused on mastering neural network fundamentals from scratch!*  
